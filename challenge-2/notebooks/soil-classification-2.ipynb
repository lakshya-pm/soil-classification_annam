{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Author: Lakshya Marwaha\n",
    "Team Name: Individual\n",
    "Team Members: -\n",
    "Leaderboard Rank: 44\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Imports and Data Loading\n",
    "\n",
    "Import additional libraries for deep learning, feature extraction, and SVM.  \n",
    "Load the training and test data, and display basic dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T19:43:21.298582Z",
     "iopub.status.busy": "2025-05-24T19:43:21.298014Z",
     "iopub.status.idle": "2025-05-24T19:43:21.310692Z",
     "shell.execute_reply": "2025-05-24T19:43:21.309978Z",
     "shell.execute_reply.started": "2025-05-24T19:43:21.298561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced imports for one-class classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv')\n",
    "print(f\"Training samples: {len(train_df)} (all soil images)\")\n",
    "print(f\"Test samples: {len(test_df)} (soil + non-soil mix)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extractor Definition\n",
    "\n",
    "Define a feature extractor using a ConvNeXt-Base backbone (from timm) to extract high-level features from soil images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T19:43:44.910193Z",
     "iopub.status.busy": "2025-05-24T19:43:44.909624Z",
     "iopub.status.idle": "2025-05-24T19:43:46.926004Z",
     "shell.execute_reply": "2025-05-24T19:43:46.925228Z",
     "shell.execute_reply.started": "2025-05-24T19:43:44.910173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor ready (ConvNeXt-Base)\n"
     ]
    }
   ],
   "source": [
    "class SoilFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name='convnext_base'):\n",
    "        super().__init__()\n",
    "        # Use proven ConvNeXt from Task 1\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = SoilFeatureExtractor('convnext_base').to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "print(\"Feature extractor ready (ConvNeXt-Base)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class and Transforms\n",
    "\n",
    "Define a custom PyTorch Dataset for loading and transforming images, and set up image preprocessing transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T19:44:24.378169Z",
     "iopub.status.busy": "2025-05-24T19:44:24.377390Z",
     "iopub.status.idle": "2025-05-24T19:44:24.386373Z",
     "shell.execute_reply": "2025-05-24T19:44:24.385720Z",
     "shell.execute_reply.started": "2025-05-24T19:44:24.378142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class and transforms ready\n"
     ]
    }
   ],
   "source": [
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = f\"{self.image_dir}/{row['image_id']}\"\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image\n",
    "\n",
    "# Define transforms (proven from Task 1)\n",
    "transforms = A.Compose([\n",
    "    A.Resize(384, 384),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"Dataset class and transforms ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction from Training Data\n",
    "\n",
    "Extract features from all training images using the defined feature extractor and save them for SVM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T19:44:54.622362Z",
     "iopub.status.busy": "2025-05-24T19:44:54.621462Z",
     "iopub.status.idle": "2025-05-24T19:45:20.273132Z",
     "shell.execute_reply": "2025-05-24T19:45:20.272306Z",
     "shell.execute_reply.started": "2025-05-24T19:44:54.622299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training data (soil images)...\n",
      "Processed 0/1222 images\n",
      "Processed 160/1222 images\n",
      "Processed 320/1222 images\n",
      "Processed 480/1222 images\n",
      "Processed 640/1222 images\n",
      "Processed 800/1222 images\n",
      "Processed 960/1222 images\n",
      "Processed 1120/1222 images\n",
      "Training features shape: (1222, 1024)\n",
      "Feature extraction complete!\n"
     ]
    }
   ],
   "source": [
    "def extract_features(df, image_dir, model, batch_size=32):\n",
    "    dataset = SoilDataset(df, image_dir, transforms)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    features = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, images in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            batch_features = model(images)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processed {batch_idx * batch_size}/{len(dataset)} images\")\n",
    "    \n",
    "    return np.vstack(features)\n",
    "\n",
    "# Extract features from training data (only soil images)\n",
    "print(\"Extracting features from training data (soil images)...\")\n",
    "train_features = extract_features(train_df, '/kaggle/input/soil-classification-part-2/soil_competition-2025/train', \n",
    "                                 feature_extractor, batch_size=16)\n",
    "\n",
    "print(f\"Training features shape: {train_features.shape}\")\n",
    "print(\"Feature extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-Class SVM Training\n",
    "\n",
    "Normalize the extracted features and train a One-Class SVM to distinguish soil from non-soil images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T19:46:14.309419Z",
     "iopub.status.busy": "2025-05-24T19:46:14.309108Z",
     "iopub.status.idle": "2025-05-24T19:46:14.536940Z",
     "shell.execute_reply": "2025-05-24T19:46:14.536255Z",
     "shell.execute_reply.started": "2025-05-24T19:46:14.309392Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training One-Class SVM...\n",
      "One-Class SVM training complete!\n",
      "Support vectors: [178]\n"
     ]
    }
   ],
   "source": [
    "# Normalize features (critical for SVM)\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "\n",
    "# Train One-Class SVM (as recommended in search results)\n",
    "print(\"Training One-Class SVM...\")\n",
    "\n",
    "# nu parameter: expected proportion of outliers in test data\n",
    "# Start with 0.1 (10% outliers) and tune if needed\n",
    "one_class_svm = OneClassSVM(\n",
    "    kernel='rbf',           # RBF kernel works well for image features\n",
    "    gamma='scale',          # Auto-scale gamma\n",
    "    nu=0.1,                # Expect ~10% outliers (non-soil images)\n",
    "    cache_size=1000        # Increase cache for faster training\n",
    ")\n",
    "\n",
    "# Fit on soil features only (as per search results methodology)\n",
    "one_class_svm.fit(train_features_scaled)\n",
    "\n",
    "print(\"One-Class SVM training complete!\")\n",
    "print(f\"Support vectors: {one_class_svm.n_support_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Pipeline: Feature Extraction, SVM Training, and Submission\n",
    "\n",
    "Apply fixes to the dataset class and feature extraction, retrain the SVM, extract test features, make predictions, and generate the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:07:48.683114Z",
     "iopub.status.busy": "2025-05-24T20:07:48.682286Z",
     "iopub.status.idle": "2025-05-24T20:08:46.602047Z",
     "shell.execute_reply": "2025-05-24T20:08:46.601204Z",
     "shell.execute_reply.started": "2025-05-24T20:07:48.683084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training data...\n",
      "Processed 0/1222 images\n",
      "Processed 160/1222 images\n",
      "Processed 320/1222 images\n",
      "Processed 480/1222 images\n",
      "Processed 640/1222 images\n",
      "Processed 800/1222 images\n",
      "Processed 960/1222 images\n",
      "Processed 1120/1222 images\n",
      "Training features shape: (1222, 1024)\n",
      "Training One-Class SVM...\n",
      "One-Class SVM training complete!\n",
      "Extracting features from test data...\n",
      "Processed 0/967 images\n",
      "Processed 160/967 images\n",
      "Processed 320/967 images\n",
      "Processed 480/967 images\n",
      "Processed 640/967 images\n",
      "Processed 800/967 images\n",
      "Processed 960/967 images\n",
      "Submission created successfully!\n",
      "Prediction distribution:\n",
      "  Soil (Class 1): 307 images\n",
      "  Non-soil (Class 0): 660 images\n"
     ]
    }
   ],
   "source": [
    "# Fix 1: Correct the Dataset Class Path Issue\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Fix: Remove the extra folder and .jpg extension\n",
    "        img_path = f\"{self.image_dir}/{row['image_id']}\"\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = np.array(image)\n",
    "            \n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "                \n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a dummy image if file not found\n",
    "            dummy_image = np.zeros((384, 384, 3), dtype=np.uint8)\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=dummy_image)\n",
    "                return augmented['image']\n",
    "            return dummy_image\n",
    "\n",
    "# Fix 2: Updated Feature Extraction with No Multiprocessing\n",
    "def extract_features(df, image_dir, model, batch_size=32):\n",
    "    # Set num_workers=0 to avoid multiprocessing issues\n",
    "    dataset = SoilDataset(df, image_dir, transforms)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    features = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, images in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            batch_features = model(images)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processed {batch_idx * batch_size}/{len(dataset)} images\")\n",
    "    \n",
    "    return np.vstack(features)\n",
    "\n",
    "# Fix 3: Continue with One-Class SVM Training\n",
    "print(\"Extracting features from training data...\")\n",
    "train_features = extract_features(train_df, '/kaggle/input/soil-classification-part-2/soil_competition-2025/train', \n",
    "                                 feature_extractor, batch_size=16)\n",
    "\n",
    "print(f\"Training features shape: {train_features.shape}\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "\n",
    "# Train One-Class SVM\n",
    "print(\"Training One-Class SVM...\")\n",
    "one_class_svm = OneClassSVM(\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    nu=0.1,\n",
    "    cache_size=1000\n",
    ")\n",
    "\n",
    "one_class_svm.fit(train_features_scaled)\n",
    "print(\"One-Class SVM training complete!\")\n",
    "\n",
    "# Extract test features\n",
    "print(\"Extracting features from test data...\")\n",
    "test_features = extract_features(test_df, '/kaggle/input/soil-classification-part-2/soil_competition-2025/test', \n",
    "                                feature_extractor, batch_size=16)\n",
    "\n",
    "# Make predictions\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "predictions = one_class_svm.predict(test_features_scaled)\n",
    "\n",
    "# Convert to binary labels (1 for soil, 0 for non-soil)\n",
    "binary_predictions = np.where(predictions == 1, 1, 0)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_df['image_id'],\n",
    "    'label': binary_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission created successfully!\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(f\"  Soil (Class 1): {np.sum(binary_predictions == 1)} images\")\n",
    "print(f\"  Non-soil (Class 0): {np.sum(binary_predictions == 0)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimized Ensemble Approach for F1=1.0\n",
    "\n",
    "Implement an ensemble of One-Class SVMs with different `nu` values, combine their predictions, and optimize the threshold for perfect F1-score.  \n",
    "Generate and save the optimized submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Implementing optimized approach for F1=1.0000...\n",
      "Training ensemble of One-Class SVMs...\n",
      "Training model with nu=0.03\n",
      "Training model with nu=0.05\n",
      "Training model with nu=0.08\n",
      "Training model with nu=0.1\n",
      "Training model with nu=0.12\n",
      "Ensemble complete! Used 5 models\n",
      "Threshold: -0.0240\n",
      "\n",
      "Optimized submission created!\n",
      "Previous F1-score: 0.8832\n",
      "New prediction distribution:\n",
      "  Soil (Class 1): 324 images (33.5%)\n",
      "  Non-soil (Class 0): 643 images (66.5%)\n",
      "Changed predictions: 17/967 (1.8%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the missing MultiLayerFeatureExtractor class\n",
    "class MultiLayerFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convnext = timm.create_model('convnext_base', pretrained=True, num_classes=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from multiple stages\n",
    "        features = []\n",
    "        x = self.convnext.stem(x)\n",
    "        \n",
    "        for i, stage in enumerate(self.convnext.stages):\n",
    "            x = stage(x)\n",
    "            if i >= 2:  # Use later stages for richer features\n",
    "                pooled = F.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "                features.append(pooled)\n",
    "        \n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "# Simplified but effective optimization approach\n",
    "def achieve_perfect_score_v2():\n",
    "    print(\"Implementing optimized approach for F1=1.0000...\")\n",
    "    \n",
    "    # Strategy 1: Multiple nu values ensemble\n",
    "    print(\"Training ensemble of One-Class SVMs...\")\n",
    "    ensemble_models = []\n",
    "    nu_values = [0.03, 0.05, 0.08, 0.1, 0.12]  # More conservative nu values\n",
    "    \n",
    "    for nu in nu_values:\n",
    "        print(f\"Training model with nu={nu}\")\n",
    "        model = OneClassSVM(\n",
    "            kernel='rbf', \n",
    "            nu=nu, \n",
    "            gamma='scale', \n",
    "            cache_size=1000\n",
    "        )\n",
    "        model.fit(train_features_scaled)\n",
    "        ensemble_models.append((model, nu))\n",
    "    \n",
    "    # Strategy 2: Weighted ensemble prediction\n",
    "    all_scores = []\n",
    "    weights = []\n",
    "    \n",
    "    for model, nu in ensemble_models:\n",
    "        scores = model.decision_function(test_features_scaled)\n",
    "        all_scores.append(scores)\n",
    "        # Lower nu gets higher weight (more conservative models)\n",
    "        weights.append(1.0 / nu)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # Weighted ensemble scores\n",
    "    ensemble_scores = np.average(all_scores, axis=0, weights=weights)\n",
    "    \n",
    "    # Strategy 3: Optimized threshold using training data\n",
    "    train_ensemble_scores = []\n",
    "    for model, nu in ensemble_models:\n",
    "        train_scores = model.decision_function(train_features_scaled)\n",
    "        train_ensemble_scores.append(train_scores)\n",
    "    \n",
    "    train_ensemble_avg = np.average(train_ensemble_scores, axis=0, weights=weights)\n",
    "    \n",
    "    # Use 5th percentile as threshold (very conservative)\n",
    "    threshold = np.percentile(train_ensemble_avg, 5)\n",
    "    \n",
    "    # Final predictions\n",
    "    final_predictions = (ensemble_scores > threshold).astype(int)\n",
    "    \n",
    "    print(f\"Ensemble complete! Used {len(ensemble_models)} models\")\n",
    "    print(f\"Threshold: {threshold:.4f}\")\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# Execute the simplified optimization\n",
    "perfect_predictions = achieve_perfect_score_v2()\n",
    "\n",
    "# Create optimized submission\n",
    "optimized_submission = pd.DataFrame({\n",
    "    'image_id': test_df['image_id'],\n",
    "    'label': perfect_predictions\n",
    "})\n",
    "\n",
    "optimized_submission.to_csv('optimized_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nOptimized submission created!\")\n",
    "print(f\"Previous F1-score: 0.8832\")\n",
    "print(f\"New prediction distribution:\")\n",
    "print(f\"  Soil (Class 1): {np.sum(perfect_predictions == 1)} images ({np.sum(perfect_predictions == 1)/len(perfect_predictions)*100:.1f}%)\")\n",
    "print(f\"  Non-soil (Class 0): {np.sum(perfect_predictions == 0)} images ({np.sum(perfect_predictions == 0)/len(perfect_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Compare with previous predictions\n",
    "if 'binary_predictions' in globals():\n",
    "    changed_predictions = np.sum(perfect_predictions != binary_predictions)\n",
    "    print(f\"Changed predictions: {changed_predictions}/{len(perfect_predictions)} ({changed_predictions/len(perfect_predictions)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12412856,
     "sourceId": 102966,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
